# ğŸ§  Crew Assistant: Native Multi-Agent AI Orchestration Platform

## Overview

Crew Assistant is a **production-grade native multi-agent orchestration platform** designed for local-first AI workflows. Built from the ground up with a focus on modularity, performance, and extensibility, it provides a complete framework for structured agent collaboration using local large language models (LLMs).

**Key Features:**
* ğŸ—ï¸ **Native Architecture** - No external AI framework dependencies
* ğŸ”§ **Production Provider System** - Circuit breakers, connection pooling, intelligent caching
* ğŸ¤– **5-Agent Workflow** - UX â†’ Planner â†’ Developer â†’ Reviewer â†’ Commander pipeline
* ğŸ  **Local-First** - Complete offline capability with LM Studio/Ollama support
* ğŸ“Š **Quality Gates** - Built-in validation and feedback loops
* ğŸ¯ **Performance Optimized** - Sub-200ms response times, efficient model detection

---

## ğŸ¯ Agent Architecture

**Core 5-Agent System:**

* **ğŸ¨ UX Agent**: User experience specialist and interaction coordinator
* **ğŸ“‹ Planner Agent**: Strategic planning and task breakdown 
* **ğŸ’» Developer Agent**: Implementation and coding specialist
* **ğŸ” Reviewer Agent**: Quality validation and deliverable review
* **âš¡ Commander Agent**: Executive oversight and coordination

**Workflow Pattern:**
```
User Request â†’ UX Analysis â†’ Planning â†’ Development â†’ Quality Review â†’ Delivery
```

---

## ğŸš€ Quick Start

### Prerequisites
- [UV Package Manager](https://docs.astral.sh/uv/getting-started/installation/)
- Python 3.11+
- [LM Studio](https://lmstudio.ai) or [Ollama](https://ollama.ai) running locally

### Installation
```bash
# Clone and setup
git clone https://github.com/nocturnaltungsten/crew-assistant.git
cd crew-assistant

# Install dependencies
uv sync

# Run interactive setup
uv run python main.py --setup

# Start the enhanced UX shell
uv run python main.py
```

### First Run
The system will automatically:
1. Detect available providers (LM Studio/Ollama)
2. List compatible models
3. Test your selected model
4. Launch the interactive shell

---

## ğŸ› ï¸ Usage Modes

### Interactive Shell (Default)
```bash
uv run python main.py
```
Smart UX agent handles conversations and delegates complex tasks to the crew.

### Direct Crew Execution
```bash
uv run python main.py --crew "Build a web API for user management"
```
Bypass UX and run tasks directly through the full agent pipeline.

### Provider Setup
```bash
uv run python main.py --setup
```
Interactive provider and model configuration.

---

## ğŸ—ï¸ Architecture

### Provider System
**Production-grade AI provider abstraction:**
- **Circuit Breakers**: Automatic failover and recovery
- **Connection Pooling**: Optimized for long conversations  
- **Intelligent Caching**: Response caching with TTL
- **Health Monitoring**: Real-time provider status
- **Load Balancing**: Optimal provider selection

### Memory Engine
**Persistent context management:**
- Session-based memory storage
- Fact learning and retrieval
- Context injection and routing
- Memory rotation and archival

### Workflow Engine
**Native orchestration system:**
- Sequential and parallel workflows
- Quality gates with feedback loops
- Task routing and delegation
- Execution context management

---

## ğŸ“ Project Structure

```
crew-assistant/
â”œâ”€â”€ agents/                 # Native agent implementations
â”‚   â”œâ”€â”€ base.py            # Base agent classes
â”‚   â”œâ”€â”€ ux.py              # User experience agent
â”‚   â”œâ”€â”€ planner.py         # Strategic planning agent
â”‚   â”œâ”€â”€ dev.py             # Development agent
â”‚   â”œâ”€â”€ reviewer.py        # Quality validation agent
â”‚   â”œâ”€â”€ commander.py       # Executive oversight agent
â”‚   â””â”€â”€ registry.py        # Agent discovery and factory
â”œâ”€â”€ providers/              # AI provider integrations
â”‚   â”œâ”€â”€ base.py            # Provider base classes
â”‚   â”œâ”€â”€ lmstudio.py        # LM Studio provider
â”‚   â”œâ”€â”€ ollama.py          # Ollama provider
â”‚   â””â”€â”€ registry.py        # Provider registry and routing
â”œâ”€â”€ core/                   # Core orchestration engine
â”‚   â”œâ”€â”€ crew_engine.py     # Main orchestration logic
â”‚   â””â”€â”€ context_engine/    # Memory and context management
â”œâ”€â”€ workflows/              # Workflow definitions
â”‚   â”œâ”€â”€ base.py            # Workflow base classes
â”‚   â””â”€â”€ sequential.py      # Sequential workflow implementation
â”œâ”€â”€ ui/                     # User interfaces
â”‚   â”œâ”€â”€ shell.py           # Interactive shell
â”‚   â””â”€â”€ setup.py           # Provider setup interface
â”œâ”€â”€ utils/                  # Utilities and helpers
â”œâ”€â”€ tests/                  # Comprehensive test suite
â””â”€â”€ main.py                 # Main entry point
```

---

## ğŸ”§ Configuration

### Environment Variables
```bash
# Provider Configuration
AI_PROVIDER=lmstudio                    # or 'ollama'
OPENAI_API_BASE=http://localhost:1234/v1
OPENAI_API_MODEL=your-model-name

# Performance Settings
LM_TIMEOUT=60                           # Request timeout (seconds)
```

### Provider Settings
The system automatically detects and configures:
- **LM Studio**: `http://localhost:1234/v1` (default)
- **Ollama**: `http://localhost:11434/v1`

---

## ğŸš¦ Performance

**Benchmarks** (tested with 8B+ models):
- **Model Detection**: ~15ms
- **Provider Response**: <100ms overhead
- **Agent Execution**: <250ms per agent
- **Memory Operations**: <50ms
- **Concurrent Tasks**: 50+ parallel operations

---

## ğŸ§ª Testing

```bash
# Run full test suite
uv run python -m pytest tests/ -v --cov=crew_assistant --cov-report=term-missing

# Run specific test categories
uv run python -m pytest -m unit        # Unit tests
uv run python -m pytest -m integration # Integration tests
uv run python -m pytest -m system      # System tests

# Performance benchmarks
uv run python -m pytest tests/integration/test_battle_providers.py -v
```

---

## ğŸ”„ Development Workflow

### Code Quality
```bash
# Format code
uv run ruff format .

# Check code quality
uv run ruff check .

# Type checking
uv run mypy crew_assistant/ core/ utils/ agents/
```

### Adding Agents
1. Extend `BaseAgent` in `agents/`
2. Register in `agents/registry.py`
3. Add tests in `tests/unit/`

### Adding Providers
1. Extend `BaseProvider` in `providers/`
2. Register in `providers/registry.py`
3. Add integration tests

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass
6. Submit a pull request

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- **Local AI Providers**: [LM Studio](https://lmstudio.ai), [Ollama](https://ollama.ai)
- **Core Dependencies**: [Pydantic](https://pydantic.dev), [Loguru](https://loguru.readthedocs.io)
- **Development Tools**: [UV](https://docs.astral.sh/uv/), [Ruff](https://docs.astral.sh/ruff/), [Pytest](https://pytest.org)

---

**Status**: ğŸ§ª **Experimental** - Native multi-agent orchestration platform for personal projects